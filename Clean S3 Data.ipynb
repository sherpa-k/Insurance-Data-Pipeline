{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c5bb82-47d5-4259-8272-e0d5b18cef98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[52]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c6e37d-3a5e-4d05-9921-514cb3a59021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to S3 bucket\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA6QTNYUGU6GC7ZBAD\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5c86c2-378c-409a-92bf-e007a777c0ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_missing(df):\n",
    "    # Print missing values per column\n",
    "    from pyspark.sql.functions import isnan, count, col, when\n",
    "    n_df = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    total = 0\n",
    "    for c in n_df.columns:\n",
    "        i = n_df.collect()[0][c]\n",
    "        total = total + i\n",
    "    if total > 0:\n",
    "        return total, df.count()\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba411e9-223f-4abf-8720-73b3a00925ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_missing(df):\n",
    "    df = df.replace(float('nan'), None)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fa99ff-31a0-4124-a942-4f9636a9c216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claims data has 30 missing values, which is 42.86% of data\ndisease has no missing values.\ngroup has no missing values.\nhospital data has 4 missing values, which is 20.0% of data\ngrpsubgrp has no missing values.\nPatient_records data has 19 missing values, which is 27.14% of data\nsubgroup has no missing values.\nsubscriber data has 36 missing values, which is 36.0% of data\n"
     ]
    }
   ],
   "source": [
    "# Create loop to read all tables\n",
    "input_path = 's3://kalsangsbucket/input-data/'\n",
    "file_list = [\n",
    "    'claims.json',\n",
    "    'disease.csv',\n",
    "    'group.csv',\n",
    "    'hospital.csv',\n",
    "    'grpsubgrp.csv',\n",
    "    'Patient_records.csv',\n",
    "    'subgroup.csv',\n",
    "    'subscriber.csv'\n",
    "]\n",
    "output_path = 's3://kalsangsbucket/cleaned_data/'\n",
    "\n",
    "\n",
    "# Find the missing values in the datasets\n",
    "for f in file_list:\n",
    "    # Depending on file extension we will read either json or csv\n",
    "    if f.split(\".\")[1] == 'json': \n",
    "        df = spark.read.option(\"header\", True).json(input_path+f)\n",
    "    else:\n",
    "        df = spark.read.option(\"header\", True).csv(input_path+f)\n",
    "    missing_ct, row_ct = check_missing(df) # Call function to see missing values\n",
    "    if missing_ct > 0:\n",
    "        print(f.split(\".\")[0], \"data has\", str(missing_ct), \"missing values, which is\", str(round((missing_ct/row_ct)*100, 2)) + \"% of data\")\n",
    "        # remove missing values\n",
    "        clean_df = remove_missing(df)\n",
    "        if f.split(\".\")[1] == 'json':\n",
    "            clean_df.write.option('header', True).json(output_path+f)\n",
    "        else:\n",
    "            clean_df.write.option('header', True).csv(output_path+f)\n",
    "    else:\n",
    "        print(f.split(\".\")[0], \"has no missing values.\")\n",
    "        if f.split(\".\")[1] == 'json':\n",
    "            df.write.option('header', True).json(output_path+f)\n",
    "        else:\n",
    "            df.write.option('header', True).csv(output_path+f)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clean S3 Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
